library('magrittr')
library('cluster')
library('dplyr')

write.fddata <- function(dlist, stem='food-dmnd-price', byname=FALSE)
{
    ## dlist:  list of data subsets (e.g., as generated by split(...))
    ## stem:  filename stem
    ## byname:  TRUE - name files using names(dlist).
    ##          FALSE - name files using sequential numbers.
    if(byname) {
        ## Get names of groupings.  Replace whitespace and underscores with hyphens.
        itervals <- gsub('[[:space:]_]+', '-', names(dlist))
    }
    else {
        itervals <- seq_along(names(dlist))
    }

    for(iter in itervals) {
        ## construct filename
        fn <- paste(stem,'.',iter,'.csv', sep='')
        write.csv(dlist[[iter]], file = fn, row.names=FALSE)
    }
}

recursive.partition <- function(input.data, cluster.vars, min.members=5)
{
    ## Partition the input data into clusters with a given minimum
    ## number of members per cluster.  This turns out to be kind of
    ## hard to do because the structure produced by the clustering
    ## algorithm isn't conducive to finding the cluster that a
    ## too-small cluster was split from.  Instead, we find a
    ## dendrogram cut that produces all clusters larger than the
    ## minimum.  Then we recursively partition all of the clusters
    ## that are large enough that they could be split in two.

    ## This function returns a list of data frames, with each list
    ## item being a single cluster.  This irretrievably scrambles the
    ## order of the rows, so if recovering the original order is
    ## important, include an ID column.

    minsplit <- 2*min.members           # clusters smaller than this can't be split

    if(nrow(input.data) < minsplit) {
        ## no split possible.  We will just
        cluster.list <- list(input.data)
    }
    else {
        ## run the clustering analysis on the input data
        dv <- diana(input.data[,cluster.vars], metric='manhattan', stand=TRUE)
        ## find the largest number of clusters that will ensure that
        ## no cluster has less than min.members.
        k <- floor(nrow(input.data) / min.members)
        count <- cutree(dv,k) %>% table %>% min    # count number in each cluster and find the smallest.
        while(count < min.members) {
            ## This loop is guaranteed to terminate because when k=1
            ## all the rows go into a single cluster.  Also, we could
            ## do a binary search here, but cutting the tree is fast,
            ## so it's easier just to scan.
            k <- k-1
            count <- cutree(dv,k) %>% table %>% min
        }

        if(k==1) {
            ## can't split this cluster without creating orphans
            cluster.list <- list(input.data)
        }
        else {
            cluster.list <- cutree(dv,k) %>% split(input.data, .)

            ## check if any clusters have more than minsplit members
            if(any(sapply(cluster.list, nrow) >= minsplit)) {
                ## apply this function recursively to everything in the
                ## list.  It will be a no-op on clusters that are too
                ## small to split recursively.
                cluster.list.list <- lapply(cluster.list,
                                            . %>% recursive.partition(cluster.vars, min.members))
                cluster.list <- unlist(cluster.list.list, recursive=FALSE)
            }
        }
    }

    ## We now have a list of one or more data frames, as promised.  As
    ## an added bonus, the names() attribute of the list is the
    ## cluster number.  This number gets appended in a predictable way
    ## when we recurse, so if the first cluster gets split into 3
    ## subclusters, they will be 1.1, 1.2, and 1.3.  If the second of
    ## those is further split in two, those will end up being 1.2.1
    ## and 1.2.2, and so on.
    cluster.list
}


assign.sigma.Q <- function(input.data, min.group=5)
{
    ## assign sigma values for Qs and Qn in an input data set.  We do
    ## this by clustering the input on Ps, Pn, and Y and then taking
    ## the variance of the Qs and Qn in each cluster.

    ## assign a sequence number to each row so that we can sort them
    ## back into their original order when we're done.
    input.data$ID <- seq(1,nrow(input.data))
    cluster.vars <- c('gdp_pcap_thous2005usd', 'ns_usd_p1000cal', 's_usd_p1000cal')
    cluster.list <- recursive.partition(input.data, cluster.vars, min.group)

    ## Add the cluster identifier to each data frame in the list.  See
    ## note at the end of recursive.partition().
    for(clus in names(cluster.list)) {cluster.list[[clus]]$clusterID <- clus}

    ## calcuate the desired variances for each group
    cluster.list <- lapply(cluster.list, . %>% mutate(sig2Qn = var(ns_cal_pcap_day_thous),
                                                      sig2Qs = var(s_cal_pcap_day_thous)))
    ## put back in master list and rearrange
    new.data <- do.call(rbind,cluster.list) %>% arrange(ID)

    ## Drop the ID variable
    new.data$ID <- NULL

    new.data
}


calc.pop.weight <- function(input.data)
{
    ## Calculate a weight factor based on the population.
    popmax <- max(input.data$pop_thous)
    mutate(input.data, weight=pop_thous/popmax)
}

create.xval.data <- function(alldata, outdir=NULL)
{
    ## Create and save the cross-validation data sets.  We create two such sets:
    ##   1) Split by year:  year > 2000 goes in the testing set
    ##   2) Split by region:  10 randomly-selected regions go in the testing set
    ## Data are returned in a list.  If outdir is specified, they will
    ## also be written to output files in that directory.

    ## Randomly (but repeatably) select regions for the regional testing set
    set.seed(8675309)
    test.rgns <- sample(unique(as.character(alldata$GCAM_region_name)), 10)
    ## With the standard input dataset, the test regions should be:
    ## Australia_NZ
    ## European Free Trade Association
    ## South Africa
    ## USA
    ## Canada
    ## Japan
    ## South Asia
    ## Pakistan
    ## Middle East
    ## China
    cat('Test regions:',test.rgns, sep='\n\t')

    xval.byyear <- split(alldata, ifelse(alldata$year > 2000, 'Testing', 'Training'))
    xval.byrgn <- split(alldata, ifelse(alldata$GCAM_region_name %in% test.rgns,
                                        'Testing', 'Training'))

    if(!is.null(outdir)) {
        write.csv(xval.byyear$Testing, file.path(outdir,'xval-byyear-tst.csv'), row.names=FALSE)
        write.csv(xval.byyear$Training, file.path(outdir,'xval-byyear-trn.csv'), row.names=FALSE)
        write.csv(xval.byrgn$Testing, file.path(outdir,'xval-byrgn-tst.csv'), row.names=FALSE)
        write.csv(xval.byrgn$Training, file.path(outdir,'xval-byrgn-trn.csv'), row.names=FALSE)
    }

    c(xval.byyear=xval.byyear, xval.byrgn=xval.byrgn)
}
