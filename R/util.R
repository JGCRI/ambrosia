
#' Write food demand data to csv files
#'
#' Write a list of data frames to files, with filenames generated either from
#' the names of the list, or by sequential numbering.
#'
#' I'll be honest with you.  I really don't remember what I was using this
#' function for.  There is nothing specific to the food demand data in it, and
#' it doesn't appear to be used anywhere else.  If I don't rediscover its
#' purpose in the next round of updates, I'm going to delete it.
#'
#' @param dlist A list of data frames.  Notionally these are subsets of a larger
#' dataset, as generated by split(...).
#' @param stem String giving the stem for the generated filenames.
#' @param byname Flag indicating whether the generated filenames should use the
#' names in \code{dlist}, or be sequentially generated.
write.fddata <- function(dlist, stem='food-dmnd-price', byname=FALSE)
{
    if(byname) {
        ## Get names of groupings.  Replace whitespace and underscores with hyphens.
        itervals <- gsub('[[:space:]_]+', '-', names(dlist))
    }
    else {
        itervals <- seq_along(names(dlist))
    }

    for(iter in itervals) {
        ## construct filename
        fn <- paste(stem,'.',iter,'.csv', sep='')
        write.csv(dlist[[iter]], file = fn, row.names=FALSE)
    }
}

#' Partition input data into clusters with a minimum number of members
#'
#' Partition the input data into clusters with a given minimum number of members
#' per cluster.  This turns out to be kind of hard to do because the structure
#' produced by the clustering algorithm isn't conducive to finding the cluster
#' that a too-small cluster was split from.  Instead, we find a dendrogram cut
#' that produces all clusters larger than the minimum.  Then we recursively
#' partition all of the clusters that are large enough that they could be split
#' in two.
#'
#' The purpose of this function is to allow us to estimate the observational
#' error in historical food demand observations.  By clustering observations
#' that have similar input values (prices, GDP) we can get something
#' approximating repeated measurements of similar situations.  Setting a minimum
#' number of members per cluster allows us to have enough measurements per
#' grouping to get a resasonable estimate of the variance.
#'
#' This function returns a list of data frames, with each list item being a
#' single cluster.  This irretrievably scrambles the order of the rows, so if
#' recovering the original order is important, include an ID column.
#'
#' @param input.data Data frame of data to be clustered.
#' @param cluster.vars Vector of names of variables to use in the clustering.
#' @param min.members Desired minimum number of members per cluster.
#' @return List of data frames, one data frame for each cluster.
#' @importFrom dplyr %>%
recursive.partition <- function(input.data, cluster.vars, min.members=5)
{
    minsplit <- 2*min.members           # clusters smaller than this can't be split

    if(nrow(input.data) < minsplit) {
        ## no split possible.  We will just
        cluster.list <- list(input.data)
    }
    else {
        ## run the clustering analysis on the input data
        dv <- cluster::diana(input.data[,cluster.vars], metric='manhattan', stand=TRUE)
        ## find the largest number of clusters that will ensure that
        ## no cluster has less than min.members.
        k <- floor(nrow(input.data) / min.members)
        count <- stats::cutree(dv,k) %>% table %>% min    # count number in each cluster and find the smallest.
        while(count < min.members) {
            ## This loop is guaranteed to terminate because when k=1
            ## all the rows go into a single cluster.  Also, we could
            ## do a binary search here, but cutting the tree is fast,
            ## so it's easier just to scan.
            k <- k-1
            count <- stats::cutree(dv,k) %>% table %>% min
        }

        if(k==1) {
            ## can't split this cluster without creating orphans
            cluster.list <- list(input.data)
        }
        else {
            cluster.list <- stats::cutree(dv,k) %>% split(input.data, .)

            ## check if any clusters have more than minsplit members
            if(any(sapply(cluster.list, nrow) >= minsplit)) {
                ## apply this function recursively to everything in the
                ## list.  It will be a no-op on clusters that are too
                ## small to split recursively.
                cluster.list.list <- lapply(cluster.list,
                                            . %>% recursive.partition(cluster.vars, min.members))
                cluster.list <- unlist(cluster.list.list, recursive=FALSE)
            }
        }
    }

    ## We now have a list of one or more data frames, as promised.  As
    ## an added bonus, the names() attribute of the list is the
    ## cluster number.  This number gets appended in a predictable way
    ## when we recurse, so if the first cluster gets split into 3
    ## subclusters, they will be 1.1, 1.2, and 1.3.  If the second of
    ## those is further split in two, those will end up being 1.2.1
    ## and 1.2.2, and so on.
    cluster.list
}


#' Assign observational errors to observed demand quantities
#'
#' Assign sigma (observational error) values for Qs and Qn in an input data set.
#' We do this by clustering the input on Ps, Pn, and Y and then taking the
#' variance of the Qs and Qn in each cluster.
#'
#' Observational errors are estimated by clustering observed data by the demand
#' model input values (i.e., prices for staple and nonstaple foods) and
#' calculating the variance of observations in each cluster.  In order for this
#' to work, you have to ensure that there are enough observations in each
#' cluster to produce a variance that is at least somewhat reliable.  The
#' tradeoff here is that the larger you make the clusters, the better the
#' variance estimate is, but less alike the observations in the cluster actually
#' are (meaning some of the variance is not observational error, but actual
#' difference in demand.  This tradeoff is controlled by the \code{min.group}
#' argument.
#'
#' @param input.data Data frame of observational input.
#' @param min.group Minimum group size for clustering
#' @importFrom dplyr %>%
#' @export
assign.sigma.Q <- function(input.data, min.group=5)
{
    ## assign a sequence number to each row so that we can sort them
    ## back into their original order when we're done.
    input.data$ID <- seq(1,nrow(input.data))
    cluster.vars <- c('gdp_pcap_thous2005usd', 'ns_usd_p1000cal', 's_usd_p1000cal')
    cluster.list <- recursive.partition(input.data, cluster.vars, min.group)

    ## Add the cluster identifier to each data frame in the list.  See
    ## note at the end of recursive.partition().
    for(clus in names(cluster.list)) {cluster.list[[clus]]$clusterID <- clus}

    ## calcuate the desired variances for each group
    cluster.list <- lapply(cluster.list, . %>% mutate(sig2Qn = var(ns_cal_pcap_day_thous),
                                                      sig2Qs = var(s_cal_pcap_day_thous)))
    ## put back in master list and rearrange
    new.data <- do.call(rbind,cluster.list) %>% dplyr::arrange(ID)

    ## Drop the ID variable
    new.data$ID <- NULL

    new.data
}

#' Calculate a weight factor based on the population.
#'
#' These weight factors can be used to give high population regions more
#' influence in the model fit relative to low population regions.
#'
#' @param input.data Data frame of food demand input data from FAO
#' @return Data frame of input data with a population weight column added.
calc.pop.weight <- function(input.data)
{
    popmax <- max(input.data$pop_thous)
    dplyr::mutate(input.data, weight=pop_thous/popmax)
}

#' Create and save validation data sets.
#'
#' We create two such sets:
#' \enumerate{
#'  \item{Split by year:  year > 2000 goes in the testing set.}
#'  \item{Split by region:  10 randomly-selected regions go in the testing set.}
#' }
#'
#' Data are returned in a list.  If outdir is specified, they will
#' also be written to output files in that directory.
#'
#'
#' The original code for randomly (but repeatably) selecting regions for the
#' regional testing set was:
#' \code{
#'     set.seed(8675309)
#'     test.rgns <- sample(unique(as.character(alldata$GCAM_region_name)), 10)
#' }
#' However, changes in certain library code (e.g. dplyr) have occasionally
#' caused the order of region names to change in the output, which causes
#' the regions selected for the test set to change.  Worse, these changes
#' depend on the version of the library installed, meaning that two users
#' running the same version of this code could get different results.  To
#' prevent this, we now set the testing regions explicitly to the set that
#' we got from the procedure above at the time we submitted the first paper.
#'
#' @param alldata Data frame of all food demand input data from FAO.
#' @param outdir Directory to write output into.  If omitted, don't write output
#' to files.
#' @return List of lists of data frames.  The first list has two datasets:
#' \code{xval.byyear} and \code{xval.byrgn}.  Each of those datasets has a
#' \code{Testing} and a \code{Training} set.
#' @export
create.xval.data <- function(alldata, outdir=NULL)
{

    test.rgns <- c('Australia_NZ', 'European Free Trade Association',
                   'South Africa', 'USA', 'Canada', 'Japan', 'South Asia',
                   'Pakistan', 'Middle East', 'China')
    cat('Test regions:',test.rgns, sep='\n\t')

    xval.byyear <- split(alldata, ifelse(alldata$year > 2000, 'Testing', 'Training'))
    xval.byrgn <- split(alldata, ifelse(alldata$GCAM_region_name %in% test.rgns,
                                        'Testing', 'Training'))

    if(!is.null(outdir)) {
        write.csv(xval.byyear$Testing, file.path(outdir,'xval-byyear-tst.csv'), row.names=FALSE)
        write.csv(xval.byyear$Training, file.path(outdir,'xval-byyear-trn.csv'), row.names=FALSE)
        write.csv(xval.byrgn$Testing, file.path(outdir,'xval-byrgn-tst.csv'), row.names=FALSE)
        write.csv(xval.byrgn$Training, file.path(outdir,'xval-byrgn-trn.csv'), row.names=FALSE)
    }

    c(xval.byyear=xval.byyear, xval.byrgn=xval.byrgn)
}
